{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(\"../input/train.csv.zip\", encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv(\"../input/test.csv.zip\", encoding=\"ISO-8859-1\")\n",
    "attribute_data = pd.read_csv('../input/attributes.csv.zip')\n",
    "df_pro_desc = pd.read_csv('../input/product_descriptions.csv.zip')\n",
    "\n",
    "\n",
    "\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "\n",
    "num_train = df_train.shape[0]\n",
    "\n",
    "# def str_stemmer(s):\n",
    "#     return \" \".join([stemmer.stem(word) for word in s.lower().split()])\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "    return sum(int(str2.find(word)>=0) for word in str1.split())\n",
    "\n",
    "\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "38c1b0a379d99b1b4adf5a4d73b703448946257e"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    txt = ''.join(character for character in text if character not in string.punctuation)\n",
    "    return txt\n",
    "\n",
    "def tokenization(text, stopwords = None):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    if stopwords:\n",
    "        tokens = [token for token in tokens if token not in stopwords]\n",
    "    return tokens\n",
    "        \n",
    "    \n",
    "def stemming(tokens):\n",
    "    new_tokens = list(map(ps.stem, tokens))\n",
    "#     new_tokens = [ps.stem(token) for token in tokens]\n",
    "    return new_tokens\n",
    "\n",
    "def lemmatizing(tokens):\n",
    "    new_tokens = list(map(wn.lemmatize, tokens))\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "38c1b0a379d99b1b4adf5a4d73b703448946257e"
   },
   "outputs": [],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob import Blobber\n",
    "classifier = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "\n",
    "def spell_correction(word):\n",
    "    return str(classifier(word).correct())\n",
    "    \n",
    "\n",
    "def correct_misspell(phrase):\n",
    "    phrase = remove_punctuation(phrase)\n",
    "    phrase = tokenization(phrase)\n",
    "    phrase = ' '.join(map(spell_correction, phrase))\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38c1b0a379d99b1b4adf5a4d73b703448946257e"
   },
   "outputs": [],
   "source": [
    "df_all['typo_corrected'] = df_all['product_description'].map(spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38c1b0a379d99b1b4adf5a4d73b703448946257e"
   },
   "outputs": [],
   "source": [
    "df_all['product_title'] = df_all['product_title'].map(correct_misspell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38c1b0a379d99b1b4adf5a4d73b703448946257e"
   },
   "outputs": [],
   "source": [
    "df_all['search_term'] = df_all['search_term'].map(correct_misspell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38c1b0a379d99b1b4adf5a4d73b703448946257e"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df_all).to_csv('hugecomputing_time.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27518eab3673eb6559af003564716ea6e8de0fbb"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    " \n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    sent1 = sent1.split(' ')\n",
    "    sent2 = sent2.split(' ')\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34b11e678f3c239403e246e996013ccc50ee0e51"
   },
   "outputs": [],
   "source": [
    "# df_all['search_term'] = df_all['search_term'].apply(lambda x:str_stem(x))\n",
    "# df_all['product_title'] = df_all['product_title'].apply(lambda x:str_stem(x))\n",
    "\n",
    "# df_all['product_description'] = df_all['product_description'].apply(lambda x:str_stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7bc630c30458de488a15ed7d298727d572861acc"
   },
   "source": [
    "miss spell words correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fb1b5fc036dbbbaad1e3ba6c6a9bbc50692af6b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fb1b5fc036dbbbaad1e3ba6c6a9bbc50692af6b"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "classifier = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "def spell_correction(word):\n",
    "    return classifier(word).correct()\n",
    "    \n",
    "\n",
    "df_all['search_term'] = df_all['search_term'].apply(spell_correction)\n",
    "df_all['product_title'] = df_all['product_title'].apply(spell_correction)\n",
    "\n",
    "df_all['product_description'] = df_all['product_description'].apply(spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00bc09794fbe040accdf758323b61dcadbf9bea0"
   },
   "outputs": [],
   "source": [
    "# from textblob import Word\n",
    "# df_all['search_term'] = df_all['search_term'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "# df_all['product_title'] = df_all['product_title'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "# df_all['product_description'] = df_all['product_description'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6832d7ae11176d4376a511b0ad8478bf032f204"
   },
   "outputs": [],
   "source": [
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c001487894bd11742245825735a0bd0fe77d5b35"
   },
   "outputs": [],
   "source": [
    "df_all[df_all.search_term.str.count('\\w+')<1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de780ccf4f739ed47abafa875ac6c368ad980065"
   },
   "outputs": [],
   "source": [
    "# df_all['frequency_search_product_desc'] = df_all.loc[:, [\"search_term\",\"product_description\"]].apply(lambda row:str_common_word(*row), axis=1)\n",
    "# df_all['frequency_search_title'] = df_all.loc[:, [\"search_term\",\"product_title\"]].apply(lambda row:str_common_word(*row), axis=1)\n",
    "\n",
    "# df_all['frequ_search_with_respect_to_sum'] = df_all.loc[:, [\"frequency_search_product_desc\",\"frequency_search_title\"]].apply(lambda row:row[0]+row[1], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36cbd681bc523f52324b8737b617ed028cb5f404"
   },
   "outputs": [],
   "source": [
    "df_all.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9d2ad9158d5bdb5873ecdd2fe689dc3df7eb6a2"
   },
   "outputs": [],
   "source": [
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "810356707c7297fbbf2f1a28b007608bdf702b05"
   },
   "outputs": [],
   "source": [
    "# df_all[\"text_rank_sp\"] = df_all.loc[:, [\"search_term\",\"product_title\"]].apply(lambda x: sentence_similarity(*x), axis=1)\n",
    "# df_all[\"text_rank_sd\"] = df_all.loc[:, [\"search_term\",\"product_description\"]].apply(lambda x: sentence_similarity(*x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "60f0cc60b8b1dbc2302069772dd31dc3fa05eea8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-642ccc8616eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all' is not defined"
     ]
    }
   ],
   "source": [
    "df_all.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95b445c9248ca6f9ca13167ea0b4584b65fadf72"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.swarmplot(x='relevance',y='text_rank_sp', data=df_all[:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92c31330f40d9a79ea5179dbddcb82781d0cd0d2"
   },
   "outputs": [],
   "source": [
    "# (df_a.product_title.str.count(\"\\\\d+\") + 1).hist(bins=30)#plot number of words in title\n",
    "# (training_data.search_term.str.count(\"\\\\d+\") + 1).hist(bins=30) #plot number of words in search query\n",
    "# (df_all.relevance.value_counts()).hist(bins=30)\n",
    "\n",
    "# sns.pairplot(df_all[:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc4b1d3cd9ea237e9d8474b9b1a1e3a56b99408f"
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english')\n",
    "\n",
    "# df_all['product_title_2'] = df_all['product_title'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "# df_all['product_description_2'] = df_all['product_description'].apply(lambda x: len([x for x in x.split() if x in stop]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "555817413b7d260e9b7ebf29013684709fc31580"
   },
   "outputs": [],
   "source": [
    "# df_all['frequency_search_product_desc2'] = df_all.loc[:, [\"search_term\",\"product_description\"]].apply(lambda row:str_common_word(*row), axis=1)\n",
    "# df_all['frequency_search_title2'] = df_all.loc[:, [\"search_term\",\"product_title\"]].apply(lambda row:str_common_word(*row), axis=1)\n",
    "\n",
    "# df_all['frequ_search_with_respect_to_sum2'] = df_all.loc[:, [\"frequency_search_product_desc\",\"frequency_search_title\"]].apply(lambda row:row[0]+row[1], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a1db8ee1e13367a4d86bc3df206a3809b8a8d39"
   },
   "outputs": [],
   "source": [
    "# df_all[\"text_rank_sp2\"] = df_all.loc[:, [\"search_term\",\"product_title\"]].apply(lambda x: sentence_similarity(*x), axis=1)\n",
    "# df_all[\"text_rank_sd2\"] = df_all.loc[:, [\"search_term\",\"product_description\"]].apply(lambda x: sentence_similarity(*x), axis=1)\n",
    "# df_all = df_all.drop(['text_rank_sd2','text_rank_sp2','frequency_search_title2','frequency_search_product_desc2', 'frequ_search_with_respect_to_sum2','product_title_2','product_description_2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "074cd4884af8bf77fa3c07b05d759f355a31ccb1"
   },
   "outputs": [],
   "source": [
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3223fc8d8b93f47edb2bfa8d64b7f35e101a6ea9"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.swarmplot(x='relevance',y='text_rank_sp', data=df_all[:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42dae845f8afee6f3570a4bd002ccfd56922d8e1"
   },
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f26e7c38dac8aac054bd5ec369cbf5de7f9d8056"
   },
   "outputs": [],
   "source": [
    "# df_all['search_term_avg'] = df_all['search_term'].apply(lambda x: avg_word(x))\n",
    "# df_all['product_description_avg'] = df_all['product_description'].apply(lambda x: avg_word(x))\n",
    "# df_all['product_title_avg'] = df_all['product_title'].apply(lambda x:avg_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa539a8984162d80100e2efbe963a93a42ed5da6"
   },
   "outputs": [],
   "source": [
    "df_all.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e50f33a85dfd18358d49aaf212ee68b46bea270a"
   },
   "outputs": [],
   "source": [
    "df_all[df_all.product_title_avg<1].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85de751e7f5074dbdb2296c37019df099c12832e"
   },
   "outputs": [],
   "source": [
    "model_data = df_all.drop(['search_term','product_title','product_description', 'frequ_search_with_respect_to_sum','distance_levistein'],axis=1)\n",
    "model_data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3777044dbc26776fa43d6455f262abbd9698508b"
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2b48929f4bb4aa1190165284598f9bb0742ddf3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_train = model_data.iloc[:num_train]\n",
    "# print('df_train',df_train)\n",
    "# df_test = model_data.iloc[num_train:]\n",
    "# print('df_test',df_test)\n",
    "# id_test = df_test['id']\n",
    "\n",
    "# y_train = df_train['relevance'].values\n",
    "# X_train = df_train.drop(['id','relevance'],axis=1).values\n",
    "# X_test = df_test.drop(['id','relevance'],axis=1).values\n",
    "\n",
    "# #### Feature to the same scale\n",
    "# scX = StandardScaler()\n",
    "# X_train = scX.fit_transform(X_train)\n",
    "# X_test = scX.fit_transform(X_test)\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators=4, max_depth=6, random_state=0)\n",
    "# clf = BaggingRegressor(rf, n_estimators=4, max_samples=0.1, random_state=25)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6838d16ee496e62613f4e6bb9b204cdb819150e"
   },
   "outputs": [],
   "source": [
    "df_all.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b2ae9d6d0f3e8a895f8e1deeccdece0d868a7c2"
   },
   "source": [
    "stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7aa64fc67109b8884a73a6d51e9ff17e35b657c1"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    txt = ''.join(character for character in text if character not in string.punctuation)\n",
    "    return txt\n",
    "\n",
    "def tokenization(text, stopwords = None):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    if stopwords:\n",
    "        tokens = [token for token in tokens if token not in stopwords]\n",
    "    return tokens\n",
    "        \n",
    "    \n",
    "def stemming(tokens):\n",
    "    new_tokens = list(map(ps.stem, tokens))\n",
    "#     new_tokens = [ps.stem(token) for token in tokens]\n",
    "    return new_tokens\n",
    "\n",
    "def lemmatizing(tokens):\n",
    "    new_tokens = list(map(wn.lemmatize, tokens))\n",
    "    return new_tokens\n",
    "\n",
    "def result(txt):\n",
    "    txt1 = remove_punctuation(txt)\n",
    "    txt1 = tokenization(txt1)\n",
    "    txt1 = stemming(txt1)\n",
    "    return txt1\n",
    "# _____________\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "92aa98f9ed032f333e862c84def73dea7e08fd07"
   },
   "outputs": [],
   "source": [
    "# df_all['search_term_tokens'] = df_all['search_term'].apply(lambda row:result(row))\n",
    "# df_all['product_description_tokens'] = df_all['product_description'].apply(lambda row:result(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
