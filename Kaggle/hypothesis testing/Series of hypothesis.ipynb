{
  "cells": [
    {
      "metadata": {
        "_uuid": "5409dbf91e6de6fc39fd002c58bb0f55dba1bcf8"
      },
      "cell_type": "markdown",
      "source": "## What is this competition all about?\n\n* Given product information such as product_description, product_title, product_uid with respect to search query we are asked to predict the relevancy score on test_set.\n* Training set contains information about product such as _id_, _product title_,  _product uid_, and _search query_ and their _relevancy score_.\n* Attribute set contains technical details about the product along with product identification number _product uid_, for most of them data aren't present.\n* Description set contains information about product description along with _product uid_ (for all).\n* Test set contains information about the product such as _id_, _product title_, _search terms_ for which _relevancy score_ have to be predicted.\n\n"
    },
    {
      "metadata": {
        "_uuid": "e3b4336bd38793d5b3337c475d2897ac9d534369"
      },
      "cell_type": "markdown",
      "source": "## Loading packages\n"
    },
    {
      "metadata": {
        "_uuid": "1720d9873a1549cd29ce34e9c821ca3628dfd3fe",
        "colab": {},
        "colab_type": "code",
        "id": "nqvGk3kkhgSv",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\n\n%matplotlib inline \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "886ee284c51eebdc8ec2553ae79b066fa1603442"
      },
      "cell_type": "markdown",
      "source": "## What is Home depot product search relevance\n\n\nShoppers rely on Home Depot’s product authority to find and buy the latest products and to get timely solutions to their home improvement needs.\nFrom installing a new ceiling fan to remodeling an entire kitchen, with the click of a mouse or tap of the screen, customers expect the correct results to their queries – quickly. \nSpeed, accuracy and delivering a frictionless customer experience are essential.\n\n\n"
    },
    {
      "metadata": {
        "_uuid": "a4804691538d1dae2c146758610078cb38873901"
      },
      "cell_type": "markdown",
      "source": "## Let's get familiar with the data!\n\n### Training data\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f535e1371b940544b5c4c1881578dbb881714fb"
      },
      "cell_type": "code",
      "source": "training_data = pd.read_csv(\"../input/train.csv\", encoding=\"ISO-8859-1\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "323297e9ef7bd783a9769afe4f2847865b568148"
      },
      "cell_type": "code",
      "source": "print(\"training data shape is:\",training_data.shape)\nprint(\"training data has empty values:\",training_data.isnull().values.any())\ntraining_data.head(3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1ec287b1abe0558945b759fdeb67df5d8e43fd47"
      },
      "cell_type": "markdown",
      "source": "As we can see there is no empty fields like NaN, but an interesting questions pop up. \n1. What is the smallest length of a sentece in the search terms.\n2. What is the biggest length in a sentence in search terms.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b29a02a072eabd0b51b621280f59192fdc09bd1e"
      },
      "cell_type": "code",
      "source": "search_terms_df = training_data[training_data.search_term.str.len() == training_data.search_term.str.len().min()]\nprint(\"Mean: {}\".format(search_terms_df.relevance.mean()))\nsearch_terms_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cbeee366cb82376b0c52f495efe23217f0b41525"
      },
      "cell_type": "code",
      "source": "search_terms_df = training_data[training_data.search_term.str.len() == training_data.search_term.str.len().max()]\nprint(\"Mean: {}\".format(search_terms_df.relevance.mean()))\nsearch_terms_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e26280d0556ec3e976c192e5dae95ad9c86e4a67"
      },
      "cell_type": "markdown",
      "source": "It appears that the following two hypothesis has to be tested\n1. More words appears in search query the higher the chance that result will be relevance\n2. Fewer words appears in search query have less relevance score\n\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "617b3218beac584fbe390680808aa1ceb46ab566"
      },
      "cell_type": "code",
      "source": "training_data['seach_query_length'] = training_data.search_term.str.len()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3739f433f8ff22d0ce77ec09cbfd58959763139c"
      },
      "cell_type": "code",
      "source": "training_data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5d9fba54ead46b9234d21bdfa1817d4f401330c0"
      },
      "cell_type": "markdown",
      "source": "It looks like there is no correlation between length of search query and relevance score\n## How much relevancy the following cases scored\n1. Search terms which consists of diggits and words (for example like \"4*8 beadboard paneling\").\n2. Search terms which  consists of diggits and in between a character (for example 3 x 2).\n3. Search terms which consists of only diggits (if there are any of them)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9240c9ea88858dbf342b328d380925aee80cc414"
      },
      "cell_type": "code",
      "source": "mask = training_data.search_term.str.contains(\"[\\d\\w]\")\nr = training_data.loc[mask, ['product_title', 'search_term','relevance']].groupby('relevance')\nr.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e29d86a7c77598b90c32beb4f3fd2e8d3b8b9dc"
      },
      "cell_type": "code",
      "source": "training_data[training_data.search_term.str.contains(\"\\d+\\w.+\\d+\")]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "230cc4ca4e06d06a56c904a3e55d236aff3423a5"
      },
      "cell_type": "code",
      "source": "training_data[training_data.search_term.str.contains(\"_\\d+\")]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd26d76b7a2dad4d390b36966214a5c9d9fb2222"
      },
      "cell_type": "code",
      "source": "training_data[training_data.search_term.str.contains(\"^\\\\d+ . \\\\d+$\")]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "230cc4ca4e06d06a56c904a3e55d236aff3423a5"
      },
      "cell_type": "code",
      "source": "# testing_data = pd.read_csv(\"../input/test.csv\", encoding=\"ISO-8859-1\")\n# attribute_data = pd.read_csv('../input/attributes.csv')\n# descriptions = pd.read_csv('../input/product_descriptions.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8d8255455a227770ba88b363dc52982f0d80a9e"
      },
      "cell_type": "code",
      "source": "import nltk\n\ndef compute_exponent_of_conv(word_dist):\n    n = len(word_dist.items())\n    r = [e for _, e in word_dist.items()]\n    return max([np.log(n) / np.log(_) for _ in r])\n\n\n# df = training_data[training_data.search_term.str.contains(\"_\\d+\")]\ndf = training_data[training_data.search_term.str.contains(\"^\\\\d+ . \\\\d+$\")]\n\nlowest_relevancy_score = df[df.relevance < 3].search_term.values.tolist()\nhigh_score = df[df.relevance == 3].search_term.values.tolist()\n\ndef make_plot(words):  \n    all_words = ''.join(words)\n    words = nltk.tokenize.word_tokenize(all_words)\n    word_dist = nltk.FreqDist(words)\n    title = compute_exponent_of_conv(word_dist)\n    word_dist.plot( title=title)\n    \n    \nmake_plot(lowest_relevancy_score)\nmake_plot(high_score)\nmake_plot(training_data[training_data.relevance == 1].search_term.values.tolist()[:8000])\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d01cbdf25af540411594beb086cf95a7ebd28a4"
      },
      "cell_type": "markdown",
      "source": "It kindly hard to say anything about this two graphs, and they are both tends to have the same rate of convergency"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac02f3eb63fb721aca064c16330750b8bcf075a3"
      },
      "cell_type": "markdown",
      "source": "# Ideas\n1. we can build tfids for each ranking and try to play with for predictions\n2. find out the most common words which has been shared\n3. from 2 and plot most common words and their sharing accross\n4. use tfids for relevance score"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c41ddfa7caa20c6a3bdde646eae4c25115941f7"
      },
      "cell_type": "code",
      "source": "total_words = training_data.search_term.unique().tolist()[:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c41ddfa7caa20c6a3bdde646eae4c25115941f7"
      },
      "cell_type": "code",
      "source": "make_plot(total_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03cc47c52b5a8093fe057d1f1efb43f4532be63c"
      },
      "cell_type": "code",
      "source": "total_words = training_data[training_data.relevance==3].search_term.unique().tolist()[:10]\nmake_plot(total_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c464de35ccc87a3e9b3b2a24136b822171c8694b"
      },
      "cell_type": "code",
      "source": "total_words = training_data[training_data.relevance==2].search_term.unique().tolist()[:10]\nmake_plot(total_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "257ac30c165a6a81c930ef19c704c13273f10f0d"
      },
      "cell_type": "code",
      "source": "total_words = training_data[training_data.relevance==1].search_term.unique().tolist()[:10]\nmake_plot(total_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "146a6dd1c2a0dab5575d3e45ca8d539a5aea818d"
      },
      "cell_type": "code",
      "source": "attributes_data = pd.read_csv(\"../input/train.csv\", encoding=\"ISO-8859-1\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b37e534beb0ed6fe9d5955167995e81c2428e23"
      },
      "cell_type": "code",
      "source": "total_words = training_data[training_data.relevance<3].search_term.unique().tolist()[:10]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68eb61a85197f6907f42c84d76ff4d62c9c2180f"
      },
      "cell_type": "markdown",
      "source": "## Interesting\nIt doesn't make sens to use much data to gathering more data than unique since it look different than above graphs. \nLets try another way to do it, how does the attributes are affecting the score based on "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b41dfda19e45c4f367ff784345ff5d1fb8519e1a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fccfd0b2d600961a9da7eb5e5251d5a5b8aaa743"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9123ed0292a0ec97b6085a607ae942e9d1bee827"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be28d49693390d3f54a087d8f7f213f98bf0816e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59ea3ba5507d2719bb9d639472cd6f7baf686e89"
      },
      "cell_type": "code",
      "source": "training_data[training_data.search_term.str.contains(\"_\\d+\")]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5ccca33255c26dfbe6b3fcbbe178283d36fd239"
      },
      "cell_type": "markdown",
      "source": "next quesiton is wheathe it has at leas one common shared word in lemmatisation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81414555f240d8a070b654864408c98f29dc85cb"
      },
      "cell_type": "markdown",
      "source": "## what can we plot\n1. number of shared words between search query and product metadata\n2. number of words in seach query\n3. number of non shared words\n4. __Number of shared words between search title and attributes __\n5. Number of shared words between shearch title and description\n6. Number of total shared words\n7. Number of tota non shared words\n8. number of common elements in tfids \n9. number of only words\n10. group relevance by part of speach like tags frequency vs relevance\n11. calculate symmetric difference between search_terms and product title store it as feature\n12. corrected jedict distance / symmetric distance\n\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ceca13c9ab0f362d1a721d9e20a9a3e12765b4a"
      },
      "cell_type": "code",
      "source": "################begin testing\n## let's create first the cleaning functions\nfrom bs4 import BeautifulSoup\nimport lxml\nimport re\nimport nltk\nfrom nltk.corpus import stopwords # Import the stop word list\nfrom nltk.metrics import edit_distance\nfrom string import punctuation\nfrom collections import Counter\nfrom nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\n\n\ndef remove_html_tag(text):\n    soup = BeautifulSoup(text, 'lxml')\n    text = soup.get_text().replace('Click here to review our return policy for additional information regarding returns', '')\n    return text\n\ndef str_stemmer(doc):\n    # split into tokens by white space\n    tokens = doc.split()\n    # remove punctuation from each token\n    table = str.maketrans('', '', punctuation)\n    tokens = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    # filter out stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [w for w in tokens if not w in stop_words]\n    # filter out short tokens\n    tokens = [word for word in tokens if len(word) > 1]\n    return ' '.join(tokens)\n\n\ndef str_stemmer_tokens(tokens):\n    # split into tokens by white space\n#     tokens = doc.split()\n    # remove punctuation from each token\n    table = str.maketrans('', '', punctuation)\n    tokens = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    # filter out stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [w for w in tokens if not w in stop_words]\n    # filter out short tokens\n    tokens = [word for word in tokens if len(word) > 1]\n    return ' '.join(tokens)\n\ndef str_stemmer_title(s):\n    return \" \".join(map(stemmer.stem, s))\n\ndef str_common_word(str1, str2):\n    whole_set = set(str1.split())\n#     return sum(int(str2.find(word)>=0) for word in whole_set)\n    return sum(int(str2.find(word)>=0) for word in whole_set)\n\n\ndef get_shared_words_mut(row_data):\n    return np.sum([str_common_word2(*row_data[:-1]), str_common_word2(*row_data[1:])])\n\n\ndef get_shared_words_imut(row_data):\n    return np.sum([str_common_word(*row_data[:-1]), str_common_word2(*row_data[1:])])\n    \nfrom nltk.corpus import brown, stopwords\nfrom nltk.cluster.util import cosine_distance\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import Counter\n\n\ndef sentence_similarity(columns,stopwords=None):\n    sent1, sent2 = columns[0], columns[1]\n    if stopwords is None:\n        stopwords = []\n \n    sent1 = [w.lower() for w in sent1]\n    sent2 = [w.lower() for w in sent2]\n \n    all_words = list(set(sent1 + sent2))\n \n    vector1 = [0] * len(all_words)\n    vector2 = [0] * len(all_words)\n \n    # build the vector for the first sentence\n    for w in sent1:\n        if w in stopwords:\n            continue\n        vector1[all_words.index(w)] += 1\n \n    # build the vector for the second sentence\n    for w in sent2:\n        if w in stopwords:\n            continue\n        vector2[all_words.index(w)] += 1\n \n    return 1 - cosine_distance(vector1, vector2)\n\ndef get_jaccard_sim(columns): \n    str1, str2 = columns[0], columns[1]\n    a = set(str1) \n    b = set(str2)\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\ndescriptions = pd.read_csv('../input/product_descriptions.csv')\ntraining_data = pd.merge(training_data, descriptions, \n                         on=\"product_uid\", how=\"left\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "baa03ac3a0ba0d6ba5158740fe36c3272e8efef1"
      },
      "cell_type": "code",
      "source": "training_data['search_term_tokens'] = training_data.search_term.str.lower().str.split()\ntraining_data['product_title_tokens'] = training_data.product_title.str.lower().str.split()\ntraining_data['product_description_tokens'] = training_data.product_description.str.lower().str.split()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ec6699e649e4b42a49ae0521f92b735c5b45db6"
      },
      "cell_type": "code",
      "source": "training_data['search_term'] = [str_stemmer_title(_) for _ in training_data.search_term_tokens.values.tolist()]\ntraining_data['product_title'] = [str_stemmer_tokens(_) for _ in training_data.product_title_tokens.values.tolist()]\ntraining_data['product_description'] = [str_stemmer_tokens(_) for _ in training_data.product_description_tokens.values.tolist()]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d673c3c173144c655178d83007d6745c0b0482e9"
      },
      "cell_type": "markdown",
      "source": "# 1. count number of shared words between search query and product metadata\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6ef29f8cdcab04d1939bc5585411b722922c5e4"
      },
      "cell_type": "code",
      "source": "def str_common_word2(str1, str2):\n    part_of_first = set(str1)\n    return sum(1 for word in str2 if word in part_of_first)\n\n\ntraining_data['c_search_term_product'] = [\n    str_common_word2(s1, s2) for s1,s2 in training_data[['search_term', 'product_description_tokens']].values.tolist()\n]\n\ntraining_data['c_search_term_desc'] = [\n    str_common_word2(s1, s2) for s1,s2 in training_data[['search_term', 'product_title']].values.tolist()\n]\n\ntraining_data['c_search_term_s'] = [\n    str_common_word2(s1, s2)+str_common_word2(s1, s3) for s1,s2,s3 in training_data[['search_term', 'product_title', 'product_description_tokens']].values.tolist()\n]\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7cef658ce53bb2722b1e8ca6f60b294bb132738e"
      },
      "cell_type": "code",
      "source": "training_data[['c_search_term_product', 'c_search_term_desc', 'relevance']].corr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2df9c40437fe9a88dd5d16ac34b4fd67ec5f071"
      },
      "cell_type": "code",
      "source": "def str_common_diff(str1, str2):\n    return len(set(str1).symmetric_difference(set(str2)))\n\ndef str_common_dij(str1, str2):\n    return len(set(str1).intersection(set(str2)))\n\ntraining_data['c_search_term_product_dij'] = [\n    str_common_dij(s1, s2)+str_common_dij(s1, s3) for s1,s2,s3 in training_data[['search_term', 'product_title', 'product_description_tokens']].values.tolist()\n]\n\ntraining_data['c_search_term_product_symm'] = [\n    str_common_diff(s1, s2)+str_common_diff(s1, s3) for s1,s2,s3 in training_data[['search_term', 'product_title','product_description_tokens']].values.tolist()\n]\n\ntraining_data[['c_search_term_product_dij', 'c_search_term_product_symm', 'c_search_term_product', 'relevance']].corr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "938ce663e6b45ad7d9fa363e935ab17466dd01d4"
      },
      "cell_type": "code",
      "source": "def get_jaccard_sim(columns): \n    str1, str2 = columns[0], columns[1]\n    a = set(str1) \n    b = set(str2)\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\n\ntraining_data['j_dis_sqt'] = [get_jaccard_sim(rows) for rows in training_data[[\"search_term_tokens\",\"product_title_tokens\"]].values]\ntraining_data['j_dis_sqd'] = [get_jaccard_sim(rows) for rows in training_data[[\"search_term_tokens\",\"product_description_tokens\"]].values]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a40bec7839475530e7ae3794ea1c9037b251dbc6"
      },
      "cell_type": "code",
      "source": "# from sklearn.model_selection import train_test_split\n# X = training_data['whole']\n# y = training_data.product_description\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "87840cb8ae326e6e17571af6be554362ab8159b9"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EDA and ML.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}