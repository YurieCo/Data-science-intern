{
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "BGi45RunhgSq",
        "_uuid": "4b828c46f913d3767c01de7e2d6192d5706e4619"
      },
      "cell_type": "markdown",
      "source": "## Data Cleaning and Shape Examining \n"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "colab_type": "code",
        "id": "a0Sqe1x7hjzu",
        "outputId": "da28f8a4-fb3c-4505-d9c5-34726bfb4246",
        "trusted": true,
        "_uuid": "2ad7ddccf61efdb2e9e7dd131aa7ae38f6dc6543"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nqvGk3kkhgSv",
        "trusted": true,
        "_uuid": "1720d9873a1549cd29ce34e9c821ca3628dfd3fe"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\nfrom nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\n\ntraining_data = pd.read_csv(\"../input/train.csv\", encoding=\"ISO-8859-1\")\ntesting_data = pd.read_csv(\"../input/test.csv\", encoding=\"ISO-8859-1\")\nattribute_data = pd.read_csv('../input/attributes.csv')\ndescriptions = pd.read_csv('../input/product_descriptions.csv')\n\ntraining_data = pd.merge(training_data, descriptions, \n                         on=\"product_uid\", how=\"left\")\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "78PcQC__hgS4",
        "outputId": "862af9df-97c3-49be-ed7e-5d19ed08c58d",
        "trusted": true,
        "_uuid": "7f54e9a8f8cc18605e917d47aca6feebfb97b151"
      },
      "cell_type": "code",
      "source": "training_data.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eXunump9hgTA",
        "_uuid": "9f21f1d572542158e7f45134c0189724c448af73"
      },
      "cell_type": "markdown",
      "source": "As in the dataset page has mentioned that it might contains some embedded html tags, let's plot and see how many in percentage, more prececily the fields 'product_description'"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "colab_type": "code",
        "id": "mD3zyodThgTD",
        "outputId": "e4611f88-2a09-4554-dcf3-f6995b02796a",
        "trusted": true,
        "_uuid": "66018a689e75b4413ae9f944f625378e42e6bb55"
      },
      "cell_type": "code",
      "source": "%matplotlib inline\ntotal_length = len(descriptions['product_description'] )\nhas_tag = sum([1 for _ in descriptions['product_description'] if '<br' in _])\nno_tags = total_length - has_tag\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.hist(x=[has_tag])\nax.ticklabel_format(useOffset=False)\n_ = plt.xlabel('number of phrase which has html tags in')\n\nplt.show()\nprint('has html tags in ',has_tag)\nprint('doesn\\'t have html tags in ', no_tags)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "p8LoHiaRhgTJ",
        "_uuid": "84fbef6e79ac8b621c58e61c715e376eb803c9a9"
      },
      "cell_type": "markdown",
      "source": "Now let's see what is the frequency of search query which include digits in it with respect to product_title which \nincludes words. As you can see most of them includes digits in search bar. \n"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "colab_type": "code",
        "id": "fcw8aOLVhgTL",
        "outputId": "5bf61497-3243-4b3d-a9e0-e4d0313c92b5",
        "trusted": true,
        "_uuid": "955978a5e37338a3f79153ca7ef1163c2deddb4a"
      },
      "cell_type": "code",
      "source": "(training_data.search_term.str.count(\"\\\\w+\") + 1).hist(bins=30) #plot number of words in search therms\n(training_data.search_term.str.count(\"\\\\d+\") + 1).hist(bins=30) #plot number of digits in search terms\n# (training_data.product_title.str.count(\"\\\\d+\") + 1).hist(bins=30)#plot number of digits in title\n\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "colab_type": "code",
        "id": "reW74Co1hgTR",
        "outputId": "5f8454a7-9975-4f09-c473-222de8f65f60",
        "trusted": true,
        "_uuid": "05b2c663b3c6598194a5a4c2453828afccf11b71"
      },
      "cell_type": "code",
      "source": "(training_data.product_title.str.count(\"\\\\w+\") + 1).hist(bins=30)#plot number of words in title\n(training_data.search_term.str.count(\"\\\\w+\") + 1).hist(bins=30) #plot number of words in search query\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "colab_type": "code",
        "id": "xRYiXtBbhgTZ",
        "outputId": "c4e90de1-b7f7-48f5-841f-9bf109eedcd1",
        "trusted": true,
        "_uuid": "2fc626dc42a2fffd5dc04518fde03f1c92617fe0"
      },
      "cell_type": "code",
      "source": "(training_data.product_title.str.count(\"\\\\d+\") + 1).hist(bins=30)#plot number of words in title\n(training_data.search_term.str.count(\"\\\\d+\") + 1).hist(bins=30) #plot number of words in search query\n\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "colab_type": "code",
        "id": "qcpD17U5hgTh",
        "outputId": "3380f8b4-4d09-4da8-f18c-fdf3802ce872",
        "trusted": true,
        "_uuid": "53f729c8f311b40e736c9b1850d327ce44b28c24"
      },
      "cell_type": "code",
      "source": "(training_data.product_description.str.count(\"\\\\d+\") + 1).hist(bins=30)\n(training_data.product_description.str.count(\"\\\\d+\\W+\\d+\") + 1).hist(bins=30)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "n-UOBiz0hgTt",
        "_uuid": "a6295bb7eca8efb42e364a95169bec7b576dcae5"
      },
      "cell_type": "markdown",
      "source": "let's plot at histogram following number of words in search query, and on the other hand relevancy score"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "colab_type": "code",
        "id": "Cegd1o0phgTv",
        "outputId": "4f66723a-9243-473b-f122-094814ea2b08",
        "trusted": true,
        "_uuid": "00266e9726a7fec706b072b6b86281cf00c27cb9"
      },
      "cell_type": "code",
      "source": "(training_data.search_term.str.count(\"\\\\w+\") + 1).hist(bins=30)\n(training_data.relevance + 1).hist(bins=30)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "txZqaXn0hgT0",
        "_uuid": "78af8c77dea8fe2179735932ef295d96433ea8cf"
      },
      "cell_type": "markdown",
      "source": "let's take a look how does the persistence of digits in the search query influence the relevancy score, from below plot it clearly that most of the search query must have between 2.0 and 3.0 "
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "colab_type": "code",
        "id": "vFzL6dsuhgT2",
        "outputId": "2b18aa72-e015-4836-9db5-76d9269a48de",
        "trusted": true,
        "_uuid": "c2a1afe75959cc52edaf987727d87cdfcc988bc9"
      },
      "cell_type": "code",
      "source": "(training_data.search_term.str.count(\"\\\\d+\")).hist(bins=30)\n(training_data.relevance ).hist(bins=30)\n(training_data.search_term.str.count(\"^\\\\d+ . \\\\d+$\")).hist(bins=30)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e4afafc8871c75d1395afe354536facd601c1fd"
      },
      "cell_type": "code",
      "source": "(training_data.search_term.str.count(\"^\\\\d+ . \\\\d+$\")).std()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1i4JNV2HhgT7",
        "_uuid": "3f4c658ac45a2da644cbf00badcc51ddb644a2d3"
      },
      "cell_type": "markdown",
      "source": "letâ€™s assume that there are zero response for null query search term\n"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "colab_type": "code",
        "id": "5bekfrZZhgT9",
        "outputId": "27b742e6-2420-48ae-c9a0-b24ffd47b8dd",
        "trusted": true,
        "_uuid": "702606c15a23a1f8bcaf369506d719a8f480ee8a"
      },
      "cell_type": "code",
      "source": "training_data[training_data.search_term.str.count('\\\\w+') < 1]\n# training_data[training_data.search_term.str.contains('^\\d+') < 1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "colab_type": "code",
        "id": "zTyh6QnHhgUC",
        "outputId": "3e28d49d-442f-4bf3-d795-c26b97a77624",
        "trusted": true,
        "_uuid": "dbd3115e7aa81bb09fde3ea283b8f31dc885de56"
      },
      "cell_type": "code",
      "source": "# an interest case can be see below, unfortunattly we cannot get rid of this element since it will make a bad impact on model\ntraining_data[training_data.product_uid==100030]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "colab_type": "code",
        "id": "qLK6NtZ7hgUJ",
        "outputId": "36df4ff3-d9c4-4a17-a38a-859fc59ea915",
        "trusted": true,
        "_uuid": "c86493591b2c80347dbb30794d17752dccb2999c"
      },
      "cell_type": "code",
      "source": "training_data[training_data.product_description.str.contains('.* x .*')].head(4) # at first it looks like nothing unsual \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "26BS8KLFhgUP",
        "_uuid": "65d0de70d59daf70f70bb539baa3de8572f988cb"
      },
      "cell_type": "markdown",
      "source": "Unfortunately, it is kind ambiguous to figure out the meaning of digits in the search context like an example below, it can mean anything. we should take care of this when cleaning context. It looks that most of the case the meaning of X is denoted the unit of measure like fit/inch/or something by something\n"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "colab_type": "code",
        "id": "BgBG3ClMhgUQ",
        "outputId": "7e8fddb7-b57b-4ed1-c1d0-120cdf68b63d",
        "trusted": true,
        "_uuid": "e4a223cf4cf2162a5e723d69bd29287e958eb434"
      },
      "cell_type": "code",
      "source": "# training_data[training_data.search_term.str.contains(\"^\\\\d+ . \\\\d+$\")].head(4)\ntraining_data[(training_data.search_term.str.contains(\"^\\\\d+ . \\\\d+$\") )& (training_data.relevance > 2)].head(4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93fedb89bc23576ba7c52cbaac874acecfdbad77"
      },
      "cell_type": "code",
      "source": "# training_data[training_data.search_term.str.contains(\"^\\\\d+ . \\\\d+$\")].head(4)\ntraining_data[(training_data.search_term.str.contains(\"^\\\\d+ . \\\\d+$\") )& (training_data.relevance > 2)].describe()\n# (training_data.search_term.str.len() & (training_data.relevance > 2)).hist(bins=30)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a7421559e56add89cd7d499835d1f10ec97d5374"
      },
      "cell_type": "code",
      "source": "training_data[(training_data.search_term.str.contains(\"^\\w+\") )& (training_data.relevance > 2)].describe()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9c2046634982ef61d6a2a0f538b3c595d013351"
      },
      "cell_type": "code",
      "source": "# exception_number = training_data[training_data.search_term.str.contains(\"^\\\\d+ . \\\\d+$\") ]['search_term'].values\n# training_data['test'] = training_data[training_data.search_term.str.contains(\"^\\\\d+ . \\\\d+$\") ]['search_term'].str.split(' ').values\n# training_data[['product_title', 'product_description', 'relevance', ]].corr()\ntraining_data.head(3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CXJ6qmA8hgUb",
        "_uuid": "97e55700afeec197d8d21bab071e5855b8b0f8c5"
      },
      "cell_type": "markdown",
      "source": "In order to apply any standard method for analysis we have to standardize metric for text fields, which we define as follows\n\n- split into tokens by white space\n- remove punctuation from each token\n- remove remaining tokens that are not alphabetic\n- filter out stop words\n- filter out short tokens\n\nand lets also create new feature in the same time which will denote our hypothesis.\n\n\\begin{equation*}\nH_1 = \\{\\ \\frac{ card(search query)}{ card(product title)} = high\\ relevance\\ score\\} \\\\\nH_2 = \\{ length(search query)\\ influence\\ relevance\\ score \\} \\\\\nH_3 = \\{ card(common\\_words(search\\ query,product\\ title,product\\ description)) = influence\\ relevance\\ score \\} \\\\\n\\end{equation*}"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "colab_type": "code",
        "id": "wNvUTEEahgUe",
        "outputId": "c43af0b5-7981-44d6-a691-49b0e53bd8f1",
        "trusted": true,
        "_uuid": "9b1eff84cc084045a3dcfb7f4ebaf34380898781"
      },
      "cell_type": "code",
      "source": "from bs4 import BeautifulSoup\nimport lxml\nimport re\nimport nltk\nfrom nltk.corpus import stopwords # Import the stop word list\nfrom nltk.metrics import edit_distance\nfrom string import punctuation\nfrom collections import Counter\nimport nltk\n\n\n\ndef remove_html_tag(text):\n    soup = BeautifulSoup(text, 'lxml')\n    text = soup.get_text().replace('Click here to review our return policy for additional information regarding returns', '')\n    return text\n\ndef str_stemmer(doc):\n    # split into tokens by white space\n    tokens = doc.split()\n    # remove punctuation from each token\n    table = str.maketrans('', '', punctuation)\n    tokens = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    # filter out stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [w for w in tokens if not w in stop_words]\n    # filter out short tokens\n    tokens = [word for word in tokens if len(word) > 1]\n    return ' '.join(tokens)\n\ndef str_stemmer_title(s):\n    return \" \".join([stemmer.stem(word) for word in s.lower().split()])\n\ndef str_common_word(str1, str2):\n    whole_set = set(str1.split())\n    return sum(int(str2.find(word)>=0) for word in whole_set)\n\n \n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Fd5F1ZGphgUk",
        "_uuid": "e330e25193001be60477ed0aa0f066ca56058873"
      },
      "cell_type": "markdown",
      "source": "Now let's build feature accordingly to \n\\begin{equation*}\nH_1, H_2, H_3\\\\\n\\end{equation*}"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed1faf63f647d3b449e07af13c07296af5db752e"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\nfrom nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\n\ntraining_data = pd.read_csv(\"../input/train.csv\", encoding=\"ISO-8859-1\")\ntesting_data = pd.read_csv(\"../input/test.csv\", encoding=\"ISO-8859-1\")\nattribute_data = pd.read_csv('../input/attributes.csv')\ndescriptions = pd.read_csv('../input/product_descriptions.csv')\n\n\nnum_train = training_data.shape[0] # memorize the boundary\n\ntraining_data = pd.concat((training_data, testing_data), axis=0, ignore_index=True, sort=False)\ntraining_data = pd.merge(training_data, descriptions, how='left', on='product_uid')\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "colab_type": "code",
        "id": "8D-bAJDchgUn",
        "outputId": "8ee2b3bd-4c08-48c0-efd5-e7076655918b",
        "trusted": true,
        "_uuid": "21ab783ea78856137325738bb6cb6ae492c64e65"
      },
      "cell_type": "code",
      "source": "############### cleaning html tags ##################\nhas_tag_in = training_data.product_description.str.contains('<br')\ntraining_data.loc[has_tag_in, 'product_description'] = training_data.loc[has_tag_in, 'product_description'].map(lambda x:remove_html_tag(x))\n###############\n\n############## apply stemming #####################\ntraining_data['search_term'] = training_data['search_term'].map(lambda x:str_stemmer_title(x))\ntraining_data['product_title'] = training_data['product_title'].map(lambda x:str_stemmer(x))\n\ntraining_data['product_description'] = training_data['product_description'].map(lambda x:str_stemmer(x))\n############## end stemming #####################\n\n############## building custome feature, let's build a few of them before compare which one is the best ###########\ntraining_data['len_of_query'] = training_data['search_term'].map(lambda x:len(x.split())).astype(np.int64)\ntraining_data['shared_words'] = training_data[['search_term','product_description', 'product_title']].apply(lambda row:sum([str_common_word(*row[:-1]), str_common_word(*row[1:])]), axis=1)\n\n# training_data['frequency_digits_in_sq']=training_data.product_description.str.count(\"\\\\d+\")\ntraining_data['frequency_words_in_sq'] = training_data.product_description.str.count(\"\\\\w+\")\ntraining_data[\"distance\"] = training_data.loc[:, [\"search_term\",\"product_title\"]].apply(lambda x: edit_distance(*x), axis=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "colab_type": "code",
        "id": "moBdZ8SchgUw",
        "outputId": "5cdbd166-8cd5-45f2-ad5d-85d8c8ec6ddb",
        "trusted": true,
        "_uuid": "58a8fd421556f14aae735f4b179486d261dc2466"
      },
      "cell_type": "code",
      "source": "#let's take a look if there is not empty search query now\n# empty_search_query = training_data[training_data.search_term.str.count('\\\\w+') < 1].values\n# print('data frame of empty seach query along with products',empty_search_query)\n# # training_data[training_data.product_uid==100030]\nis_anything_none = training_data.isnull().values.any()\nprint('presence of Nan values',  is_anything_none)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "colab_type": "code",
        "id": "Z5kKyVZZhgU1",
        "outputId": "6f7182ae-d237-490b-c432-439cd47c2f2d",
        "trusted": true,
        "_uuid": "62a89ae2e2925f9870e489dadb1b9168218b0523"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "colab_type": "code",
        "id": "SecjsKcjhgU4",
        "outputId": "bc13f12b-2ad7-45f4-c1a8-ae51719dbde7",
        "trusted": true,
        "_uuid": "5c7f97a17836a705f1e8a098ece7c171188c80a9"
      },
      "cell_type": "code",
      "source": "training_data[training_data.search_term.str.match(\"\\\\d+ x \\\\d+\") > 0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "colab_type": "code",
        "id": "r7e4vBF-hgU8",
        "outputId": "da94c93e-b9a8-4941-a9d6-88ad8017ace3",
        "trusted": true,
        "_uuid": "fd86d03f65c6298bc88b0c018db0d25f9af9a79e"
      },
      "cell_type": "code",
      "source": "training_data[['product_title','len_of_query','shared_words','frequency_words_in_sq','relevance']].corr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "colab_type": "code",
        "id": "mOetNpVMhgVC",
        "outputId": "41db7474-b780-45f4-e690-58707741c47b",
        "trusted": true,
        "_uuid": "925eb015e7d3fc199052121835b7ebc0fd9a5173"
      },
      "cell_type": "code",
      "source": "df_all = training_data.drop(['search_term','product_description','product_title'],axis=1)\n\n\ndf_train = df_all.iloc[:num_train]\ndf_test = df_all.iloc[num_train:]\nid_test = df_test['id']\n\ny_train = df_train['relevance'].values\nX_train = df_train.drop(['id','relevance'],axis=1).values\nX_test = df_test.drop(['id','relevance'],axis=1).values\n\nrf = RandomForestRegressor(n_estimators=4, max_depth=6, random_state=0)\nclf = BaggingRegressor(rf, n_estimators=4, max_samples=0.1, random_state=25)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\npd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2L4kUtZ49C_l",
        "trusted": true,
        "_uuid": "d7f2e70686dbdc26fac1709c502f5765a7a7af3a"
      },
      "cell_type": "markdown",
      "source": "Analysis of the Model\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7f23f22d4838dc39f4e01f4ec8ae5f9fc75770a1"
      },
      "cell_type": "code",
      "source": "# from nltk.metrics import edit_distance\n# from sklearn.preprocessing import StandardScaler\n\n# import numpy as np\n# import pandas as pd\n# from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n# from nltk.stem.snowball import SnowballStemmer\n\n# stemmer = SnowballStemmer('english')\n\n\n# df_train = pd.read_csv(\"../input/train.csv\", encoding=\"ISO-8859-1\")\n# df_test = pd.read_csv(\"../input/test.csv\", encoding=\"ISO-8859-1\")\n# attribute_data = pd.read_csv('../input/attributes.csv')\n# df_pro_desc = pd.read_csv('../input/product_descriptions.csv')\n\n\n\n# df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n\n# num_train = df_train.shape[0]\n\n# ###############\n# from bs4 import BeautifulSoup\n# import lxml\n# import re\n# import nltk\n# from nltk.corpus import stopwords # Import the stop word list\n# from nltk.metrics import edit_distance\n# from string import punctuation\n# from collections import Counter\n# import nltk\n\n\n# def remove_html_tag(text):\n#     soup = BeautifulSoup(text, 'lxml')\n#     text = soup.get_text().replace('Click here to review our return policy for additional information regarding returns', '')\n#     return text\n\n# def str_stemmer(doc):\n#     # split into tokens by white space\n#     tokens = doc.split()\n#     # remove punctuation from each token\n#     table = str.maketrans('', '', punctuation)\n#     tokens = [w.translate(table) for w in tokens]\n#     # remove remaining tokens that are not alphabetic\n#     tokens = [word for word in tokens if word.isalpha()]\n#     # filter out stop words\n#     stop_words = set(stopwords.words('english'))\n#     tokens = [w for w in tokens if not w in stop_words]\n#     # filter out short tokens\n#     tokens = [word for word in tokens if len(word) > 1]\n#     return ' '.join(tokens)\n\n# def str_stemmer_title(s):\n#     return \" \".join([stemmer.stem(word) for word in s.lower().split()])\n\n# def str_common_word(str1, str2):\n#     whole_set = set(str1.split())\n#     return sum(int(str2.find(word)>=0) for word in whole_set)\n\n \n\n\n\n\n# ###############\n# df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n\n# df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n\n\n# ############## apply stemming #####################\n# df_all['search_term'] = df_all['search_term'].map(lambda x:str_stemmer_title(x))\n# df_all['product_title'] = df_all['product_title'].map(lambda x:str_stemmer(x))\n\n# df_all['product_description'] = df_all['product_description'].map(lambda x:str_stemmer(x))\n# ############## end stemming #####################\n\n# ############## building custome feature, let's build a few of them before compare which one is the best ###########\n# df_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)\n# df_all['shared_words'] = df_all[['search_term','product_description', 'product_title']].apply(lambda row:sum([str_common_word(*row[:-1]), str_common_word(*row[1:])]), axis=1).astype(np.int64)\n\n# # training_data['frequency_digits_in_sq']=training_data.product_description.str.count(\"\\\\d+\")\n# df_all['frequency_words_in_sq'] = df_all.product_description.str.count(\"\\\\w+\").astype(np.int64)\n# df_all[\"distance_levistein\"] = df_all.loc[:, [\"search_term\",\"product_title\"]].apply(lambda x: edit_distance(*x), axis=1).astype(np.int64)\n\n# df_all['length in product info'] = training_data[['product_title','product_description']].apply(lambda row:sum([len(*row[:-1]), len(*row[1:])]), axis=1).astype(np.int64)\n\n# df_all = df_all.drop(['search_term','product_title','product_description'],axis=1)\n\n\n# df_train = df_all.iloc[:num_train]\n# print('df_train',df_train)\n# df_test = df_all.iloc[num_train:]\n# print('df_test',df_test)\n# id_test = df_test['id']\n\n# y_train = df_train['relevance'].values\n# X_train = df_train.drop(['id','relevance'],axis=1).values\n# X_test = df_test.drop(['id','relevance'],axis=1).values\n\n# #### Feature to the same scale\n# scX = StandardScaler()\n# X_train = scX.fit_transform(X_train)\n# X_test = scX.fit_transform(X_test)\n\n# rf = RandomForestRegressor(n_estimators=4, max_depth=6, random_state=0)\n# clf = BaggingRegressor(rf, n_estimators=4, max_samples=0.1, random_state=25)\n# clf.fit(X_train, y_train)\n# y_pred = clf.predict(X_test)\n\n# pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77a6d49ae3a271a2554129165a8a71e982b4ad3e"
      },
      "cell_type": "code",
      "source": "training_data[training_data.search_term.str.count('\\\\w+')<1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c61143c65f667a056eb4852715f05af06bf91d06"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0febb1253bafa58a82d82bbc4899016056e615ca"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "87b2af79b2c22fdd74b6b8cb06a2694ca6a941f5"
      },
      "cell_type": "code",
      "source": "# df[training_data.isnull]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "980bcb2ba3175a1234a78b208785f519ca256fd4"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "f98c4acbb92a424c1d3561b88e0bc6f279825223"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23a9fbf8c61b111881c923ea7509cf17a3a83f3c"
      },
      "cell_type": "code",
      "source": "# from nltk.metrics import edit_distance\n# from sklearn.preprocessing import StandardScaler\n\n# import numpy as np\n# import pandas as pd\n# from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n# from nltk.stem.snowball import SnowballStemmer\n\n# stemmer = SnowballStemmer('english')\n\n\n# df_train = pd.read_csv(\"../input/train.csv\", encoding=\"ISO-8859-1\")\n# df_test = pd.read_csv(\"../input/test.csv\", encoding=\"ISO-8859-1\")\n# attribute_data = pd.read_csv('../input/attributes.csv')\n# df_pro_desc = pd.read_csv('../input/product_descriptions.csv')\n\n\n\n# df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n\n# num_train = df_train.shape[0]\n\n# ###############\n# from bs4 import BeautifulSoup\n# import lxml\n# import re\n# import nltk\n# from nltk.corpus import stopwords # Import the stop word list\n# from nltk.metrics import edit_distance\n# from string import punctuation\n# from collections import Counter\n# import nltk\n\n\n# def remove_html_tag(text):\n#     soup = BeautifulSoup(text, 'lxml')\n#     text = soup.get_text().replace('Click here to review our return policy for additional information regarding returns', '')\n#     return text\n\n# def str_stemmer(doc):\n#     # split into tokens by white space\n#     tokens = doc.split()\n#     # remove punctuation from each token\n#     table = str.maketrans('', '', punctuation)\n#     tokens = [w.translate(table) for w in tokens]\n#     # remove remaining tokens that are not alphabetic\n#     tokens = [word for word in tokens if word.isalpha()]\n#     # filter out stop words\n#     stop_words = set(stopwords.words('english'))\n#     tokens = [w for w in tokens if not w in stop_words]\n#     # filter out short tokens\n#     tokens = [word for word in tokens if len(word) > 1]\n#     return ' '.join(tokens)\n\n# def str_stemmer_title(s):\n#     return \" \".join([stemmer.stem(word) for word in s.lower().split()])\n\n# def str_common_word(str1, str2):\n#     whole_set = set(str1.split())\n#     return sum(int(str2.find(word)>=0) for word in whole_set)\n\n\n\n# ###############\n# df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n\n# df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fefcb96fe21436cf0ba9e6f842ef7b9a30f2912b"
      },
      "cell_type": "markdown",
      "source": "Cleaning function"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2070769bb5138b83a7dc8695b5c24dba187228a9"
      },
      "cell_type": "code",
      "source": "# from nltk.corpus import brown, stopwords\n# from nltk.cluster.util import cosine_distance\n \n# def sentence_similarity(sent1, sent2, stopwords=None):\n#     sent1 = sent1.split(' ')\n#     sent2 = sent2.split(' ')\n#     if stopwords is None:\n#         stopwords = []\n \n#     sent1 = [w.lower() for w in sent1]\n#     sent2 = [w.lower() for w in sent2]\n \n#     all_words = list(set(sent1 + sent2))\n \n#     vector1 = [0] * len(all_words)\n#     vector2 = [0] * len(all_words)\n \n#     # build the vector for the first sentence\n#     for w in sent1:\n#         if w in stopwords:\n#             continue\n#         vector1[all_words.index(w)] += 1\n \n#     # build the vector for the second sentence\n#     for w in sent2:\n#         if w in stopwords:\n#             continue\n#         vector2[all_words.index(w)] += 1\n \n#     return 1 - cosine_distance(vector1, vector2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d2b84dc711ef2c8ef65046d05478b4022520c74"
      },
      "cell_type": "code",
      "source": "\n# ############## apply stemming #####################\n# df_all['search_term'] = df_all['search_term'].map(lambda x:str_stemmer_title(x))\n# df_all['product_title'] = df_all['product_title'].map(lambda x:str_stemmer(x))\n\n# df_all['product_description'] = df_all['product_description'].map(lambda x:str_stemmer(x))\n# ############## end stemming #####################\n\n# ############## building custome feature, let's build a few of them before compare which one is the best ###########\n# df_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)\n# df_all['shared_words'] = df_all[['search_term','product_description', 'product_title']].apply(lambda row:sum([str_common_word(*row[:-1]), str_common_word(*row[1:])]), axis=1).astype(np.int64)\n\n# # training_data['frequency_digits_in_sq']=training_data.product_description.str.count(\"\\\\d+\")\n# df_all['frequency_words_in_sq'] = df_all.product_description.str.count(\"\\\\w+\").astype(np.int64)\n# df_all[\"distance_levistein\"] = df_all.loc[:, [\"search_term\",\"product_title\"]].apply(lambda x: edit_distance(*x), axis=1).astype(np.int64)\n\n# df_all['length in product info'] = training_data[['product_title','product_description']].apply(lambda row:sum([len(*row[:-1]), len(*row[1:])]), axis=1).astype(np.int64)\n\n# df_all = df_all.drop(['search_term','product_title','product_description'],axis=1)\n\n\n# # df_train = df_all.iloc[:num_train]\n# # print('df_train',df_train)\n# # df_test = df_all.iloc[num_train:]\n# # print('df_test',df_test)\n# # id_test = df_test['id']\n\n# # y_train = df_train['relevance'].values\n# # X_train = df_train.drop(['id','relevance'],axis=1).values\n# # X_test = df_test.drop(['id','relevance'],axis=1).values\n\n# # #### Feature to the same scale\n# # scX = StandardScaler()\n# # X_train = scX.fit_transform(X_train)\n# # X_test = scX.fit_transform(X_test)\n\n# # rf = RandomForestRegressor(n_estimators=4, max_depth=6, random_state=0)\n# # clf = BaggingRegressor(rf, n_estimators=4, max_samples=0.1, random_state=25)\n# # clf.fit(X_train, y_train)\n# # y_pred = clf.predict(X_test)\n\n# # pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d702bb2b56a5d0489ff98d63333306f23a071b4e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63ab0fc654f6915b0d4733a2e421003b1843b9b2"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f4642abf3d84b8d2adc2f846e2a6e6732e9814a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea3635d67ec66f8b645a47587facf0afc166a74f"
      },
      "cell_type": "code",
      "source": "# from nltk.metrics import edit_distance\n# from sklearn.preprocessing import StandardScaler\n\n# import numpy as np\n# import pandas as pd\n# from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n# from nltk.stem.snowball import SnowballStemmer\n\n# stemmer = SnowballStemmer('english')\n\n\n# df_train = pd.read_csv(\"../input/train.csv\", encoding=\"ISO-8859-1\")\n# df_test = pd.read_csv(\"../input/test.csv\", encoding=\"ISO-8859-1\")\n# attribute_data = pd.read_csv('../input/attributes.csv')\n# df_pro_desc = pd.read_csv('../input/product_descriptions.csv')\n\n\n\n# df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n\n# num_train = df_train.shape[0]\n\n# # def str_stemmer(s):\n# #     return \" \".join([stemmer.stem(word) for word in s.lower().split()])\n\n# def str_common_word(str1, str2):\n#     return sum(int(str2.find(word)>=0) for word in str1.split())\n\n\n# df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n\n# df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n\n\n\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EDA and ML.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}